# **Assignment 2 [Total Marks - 50]**

**Objective: Setup an MLOps pipeline that allows for continuous improvement and explainability.**

**Tasks: Use the Fashion MNIST dataset for this exercise.**

**M1: Exploratory Data Analysis (EDA)**  **-**  **10 M**

Objective: Automate data analysis and generate reports for deeper insights.

Tasks:

1.  Use an EDA tool that allows for automated data analysis and report generation (e.g., Pandas Profiling, Sweetviz, or D-Tale).
2.  Provide visual summaries of the dataset, including class distribution, missing values, and feature correlations.

Deliverables:

-   A report generated by the EDA tool.
-   Screenshots showcasing key findings.

**M2: Feature Engineering & Explainability**  **-**  **10M**

Objective: Build a feature engineering pipeline with explainability visualizations.

Tasks:

1.  Feature Engineering:

-   Implement preprocessing steps such as normalization, scaling, or transformations.

3.  Explainability:

-   Use an open-source explainability library (e.g., SHAP, LIME, or InterpretML) to illustrate how each feature affects the class.
-   Use insights from explainability to refine the feature engineering pipeline.

Deliverables:

-   Feature engineering pipeline code.
-   Explainability visualizations and analysis.
-   Justification of selected features based on explainability results.

**M3: Model Selection & Hyperparameter Optimization**  **-**  **10M**

Objective: Identify the best model using AutoML and optimize its hyperparameters.

Tasks:

1.  Model Selection:

-   Use an open-source AutoML library (e.g., Auto-sklearn, H2O.ai, TPOT) to select the best-performing model.

3.  Hyperparameter Optimization:

-   Use an open-source hyperparameter optimization tool (e.g., Optuna, Hyperopt, Ray Tune) to fine-tune the selected model.

Deliverables:

-   AutoML results comparing multiple models.
-   Hyperparameter tuning logs.
-   Justification for the chosen model and hyperparameters.

**M4: Model Monitoring & Performance Tracking**  **-**  **10M**

Objective: Track and monitor model performance over time.

Tasks:

1.  Use an open-source tracking tool (e.g., MLflow, Neptune.ai, Weights & Biases) to log model performance metrics.
2.  Implement drift detection to identify when retraining is required.

Deliverables:

-   Performance tracking logs.
-   Drift detection implementation.
-   Screenshots showing model performance over multiple runs.

**M5: Final Deliverables**  **-**  **10M**

Deliverables:

1.  A zip file containing:

-   Code
-   Processed data
-   Trained model

2. A one-page summary that includes:

-   Description of the work completed.
-   Justification for the choice of libraries/tools.
-   Explanation of how each functionality contributes to MLOps best practices.

3. A screen recording (maximum 5 minutes) that:

-   Explains the work done.
-   Shows the results.